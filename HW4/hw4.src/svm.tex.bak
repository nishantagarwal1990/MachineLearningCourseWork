\section{Warm up: Support Vector Machines}
\label{sec:svm}

In class you have seen support vector machines (SVMs).  Here we will find the optimal classifier of a simple dataset by hand.  Assume that we are given the following dataset:
\begin{align*}
\mathcal{D}_+ &= \begin{bmatrix}
  3 & 1\\
  3 & -1\\
  6 & 1\\
  6 & -1
\end{bmatrix}
&& \mathcal{D}_-= \begin{bmatrix}
  1 & 0\\
  0 & 1\\
  0 & -1\\
  -1 & 0
\end{bmatrix}
\end{align*}
where $\mathcal{D}_+$ are positive examples and $\mathcal{D}_-$ are negative examples.
\begin{enumerate}
\item ~[9 points] Find the optimal hyperplane and the corresponding
  maximum margin. Which training points are the support vectors?
  
\item ~[2 points] You are given a new point $x = [1.8,1]$ with true
  label -1. Does your SVM correctly classify this point?
  
\item ~[4 points] Now let's compare this classifier to the vanilla
  Perceptron algorithm. Assume that we have the point $x = [1.9999,1]$
  with label -1. If we allow the Perceptron algorithm to run until it
  achieves 0\% error on the training set will it be gauranteed to
  classify this point correctly? Why or why not justify your answer.
\end{enumerate}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "hw4"
%%% End:
